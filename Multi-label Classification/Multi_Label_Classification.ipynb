{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbip1j9LxffG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "from sklearn.metrics import accuracy_score,hamming_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import LabelPowerset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpI7b6oOxffJ"
      },
      "outputs": [],
      "source": [
        "train_set_np=np.load(\"./training_set.npy\",allow_pickle=True)\n",
        "test_set_np=np.load(\"./testing_set.npy\",allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5w6_bCDxffJ"
      },
      "outputs": [],
      "source": [
        "train_set_pd=pd.DataFrame(train_set_np)[0:int(len(train_set_np)*0.05)]\n",
        "test_set_pd=pd.DataFrame(test_set_np)[0:int(len(test_set_np)*0.05)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S88VHYdQxffJ",
        "outputId": "efcda059-c525-405b-d44a-3eef3f3b4296"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3357, 8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_pd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wa6DWRrxffK",
        "outputId": "c395eab9-5484-438b-a33d-14701241e94f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(839, 8)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_pd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6prL6ATUxffL"
      },
      "outputs": [],
      "source": [
        "train_set_pd[5] = train_set_pd[5].astype(float)\n",
        "train_set_pd[6] = train_set_pd[6].astype(float)\n",
        "train_set_pd[7] = train_set_pd[7].astype(float)\n",
        "\n",
        "test_set_pd[5] = test_set_pd[5].astype(float)\n",
        "test_set_pd[6] = test_set_pd[6].astype(float)\n",
        "test_set_pd[7] = test_set_pd[7].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVGxvuU8xffL"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(review):\n",
        "\n",
        "    # package setting\n",
        "    stop_words = stopwords.words('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # data cleaning\n",
        "    review = re.sub(re.compile('<.*?>'), '', review) #removing html tags\n",
        "    review =  re.sub('[^A-Za-z0-9]+', ' ', review) #taking only words\n",
        "\n",
        "    # lowercase\n",
        "    review = review.lower()\n",
        "\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(review) # converts review to tokens\n",
        "\n",
        "    # stop_words removal\n",
        "    review = [word for word in tokens if word not in stop_words] #removing stop words\n",
        "\n",
        "    # lemmatization\n",
        "    review = [lemmatizer.lemmatize(word) for word in review]\n",
        "\n",
        "    # join words in preprocessed review\n",
        "    review = ' '.join(review)\n",
        "\n",
        "    return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoVGzzfdxffM"
      },
      "outputs": [],
      "source": [
        "train_set_pd['clean_text']=train_set_pd[0].apply(lambda x: data_preprocessing(x))\n",
        "test_set_pd['clean_text']=test_set_pd[0].apply(lambda x: data_preprocessing(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Frjc8oxffM"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10)\n",
        "X_train = vectorizer.fit_transform(train_set_pd['clean_text'])\n",
        "X_test = vectorizer.transform(test_set_pd['clean_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0DiX964xffN"
      },
      "outputs": [],
      "source": [
        "y_train = train_set_pd[[5,6,7]]\n",
        "y_test = test_set_pd[[5,6,7]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvyD-bDTxffN"
      },
      "outputs": [],
      "source": [
        "def build_model(model_L,mlb_estimator_L,xtrain,ytrain,xtest,ytest):\n",
        "    for model_name in model_L:\n",
        "        for mlb_estimator_name in mlb_estimator_L:\n",
        "    \n",
        "            if model_name == \"lr\":\n",
        "                model = LogisticRegression(penalty='l2',max_iter=1000)\n",
        "            elif model_name == \"nb\":\n",
        "                model = MultinomialNB()\n",
        "            elif model_name == \"rf\":\n",
        "                model = RandomForestClassifier()\n",
        "            elif model_name == \"svm\":\n",
        "                model = svm.SVC(kernel='linear')\n",
        "            \n",
        "            if mlb_estimator_name == \"binary_relevance\":\n",
        "                mlb_estimator = BinaryRelevance\n",
        "            elif mlb_estimator_name == \"classifier_chains\":\n",
        "                mlb_estimator = BinaryRelevance\n",
        "            elif mlb_estimator_name == \"labelpowerset\":\n",
        "                mlb_estimator = LabelPowerset\n",
        "    \n",
        "            clf = mlb_estimator(model)\n",
        "            clf.fit(xtrain,ytrain)\n",
        "            clf_predictions = clf.predict(xtest)\n",
        "            acc = accuracy_score(ytest,clf_predictions)\n",
        "            ham = hamming_loss(ytest,clf_predictions)\n",
        "            result = {\"model_name:\":model_name,\"mlb_estimator\":mlb_estimator_name,\"accuracy:\":acc,\"hamming_score\":ham}\n",
        "            \n",
        "            print (result)\n",
        "            \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYMbHOXNxffN"
      },
      "outputs": [],
      "source": [
        "model_L=[\"lr\",\"rf\",\"svm\"]\n",
        "mlb_estimator_L=[\"binary_relevance\",\"classifier_chains\",\"labelpowerset\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vXl_e-4xffO",
        "outputId": "5c66bf9b-c0ec-4fed-846d-520ca80dd046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_name:': 'lr', 'mlb_estimator': 'binary_relevance', 'accuracy:': 0.3909415971394517, 'hamming_score': 0.27691696464044496}\n",
            "{'model_name:': 'lr', 'mlb_estimator': 'classifier_chains', 'accuracy:': 0.3909415971394517, 'hamming_score': 0.27691696464044496}\n",
            "{'model_name:': 'lr', 'mlb_estimator': 'labelpowerset', 'accuracy:': 0.42193087008343266, 'hamming_score': 0.28843861740166865}\n",
            "{'model_name:': 'rf', 'mlb_estimator': 'binary_relevance', 'accuracy:': 0.38498212157330153, 'hamming_score': 0.278108859753675}\n",
            "{'model_name:': 'rf', 'mlb_estimator': 'classifier_chains', 'accuracy:': 0.4028605482717521, 'hamming_score': 0.2709574890742948}\n",
            "{'model_name:': 'rf', 'mlb_estimator': 'labelpowerset', 'accuracy:': 0.42073897497020263, 'hamming_score': 0.29360349622566545}\n",
            "{'model_name:': 'svm', 'mlb_estimator': 'binary_relevance', 'accuracy:': 0.38974970202622167, 'hamming_score': 0.288041319030592}\n",
            "{'model_name:': 'svm', 'mlb_estimator': 'classifier_chains', 'accuracy:': 0.38974970202622167, 'hamming_score': 0.288041319030592}\n",
            "{'model_name:': 'svm', 'mlb_estimator': 'labelpowerset', 'accuracy:': 0.4302741358760429, 'hamming_score': 0.2896305125148987}\n"
          ]
        }
      ],
      "source": [
        "build_model(model_L,mlb_estimator_L,X_train,y_train,X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-cOZZXvxffO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD11AdiIxffO"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader as gensim_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWJzSIuixffO"
      },
      "outputs": [],
      "source": [
        "embeddings = gensim_api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wduD4la4xffO"
      },
      "outputs": [],
      "source": [
        "def word2vec(df,embeddings):\n",
        "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
        "    stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
        "    for doc in df[0].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
        "        temp = pd.DataFrame() # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
        "        for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
        "            if word not in stopwords: # if word is not present in stopwords then (try)\n",
        "                try:\n",
        "                    word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
        "                    temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
        "                except:\n",
        "                    pass\n",
        "        doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
        "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
        "    return  docs_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8A4LVPXxffP"
      },
      "outputs": [],
      "source": [
        "word2vec_train=word2vec(train_set_pd,embeddings)\n",
        "word2vec_test=word2vec(test_set_pd,embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCI0OH9ZxffP",
        "outputId": "c9ae3576-5185-4be6-8496-42daa6157058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_name:': 'lr', 'mlb_estimator': 'binary_relevance', 'accuracy:': 0.3766388557806913, 'hamming_score': 0.2789034564958284}\n",
            "{'model_name:': 'lr', 'mlb_estimator': 'classifier_chains', 'accuracy:': 0.3766388557806913, 'hamming_score': 0.2789034564958284}\n",
            "{'model_name:': 'lr', 'mlb_estimator': 'labelpowerset', 'accuracy:': 0.4195470798569726, 'hamming_score': 0.2912197059992054}\n",
            "{'model_name:': 'rf', 'mlb_estimator': 'binary_relevance', 'accuracy:': 0.4052443384982122, 'hamming_score': 0.2689709972189114}\n",
            "{'model_name:': 'rf', 'mlb_estimator': 'classifier_chains', 'accuracy:': 0.3909415971394517, 'hamming_score': 0.27771156138259834}\n",
            "{'model_name:': 'rf', 'mlb_estimator': 'labelpowerset', 'accuracy:': 0.43146603098927294, 'hamming_score': 0.28684942391736196}\n",
            "{'model_name:': 'svm', 'mlb_estimator': 'binary_relevance', 'accuracy:': 0.39451728247914186, 'hamming_score': 0.2765196662693683}\n",
            "{'model_name:': 'svm', 'mlb_estimator': 'classifier_chains', 'accuracy:': 0.39451728247914186, 'hamming_score': 0.2765196662693683}\n",
            "{'model_name:': 'svm', 'mlb_estimator': 'labelpowerset', 'accuracy:': 0.42073897497020263, 'hamming_score': 0.2924116011124354}\n"
          ]
        }
      ],
      "source": [
        "build_model(model_L,mlb_estimator_L,word2vec_train,y_train,word2vec_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSHiZcFnxffP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Multi-Label Classification.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}